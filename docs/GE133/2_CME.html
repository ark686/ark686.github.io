<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>CME Research Project | ECE Documentation</title>
    <meta name="description" content="A VitePress Site">
    <meta name="generator" content="VitePress v2.0.0-alpha.12">
    <link rel="preload stylesheet" href="/docs/assets/style.ww5xl0SB.css" as="style">
    <link rel="preload stylesheet" href="/docs/vp-icons.css" as="style">
    
    <script type="module" src="/docs/assets/app.nkKO_yfb.js"></script>
    <link rel="preload" href="/docs/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/docs/assets/chunks/theme.BGfrc8Bk.js">
    <link rel="modulepreload" href="/docs/assets/chunks/framework.BDA0mgSd.js">
    <link rel="modulepreload" href="/docs/assets/GE133_2_CME.md.CmeXCLSJ.lean.js">
    <link rel="icon" href="/favicon.png">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"dark",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-1df9f90f><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-1df9f90f data-v-9f75dce3><div class="VPNavBar" data-v-9f75dce3 data-v-2a96a3d0><div class="wrapper" data-v-2a96a3d0><div class="container" data-v-2a96a3d0><div class="title" data-v-2a96a3d0><div class="VPNavBarTitle has-sidebar" data-v-2a96a3d0 data-v-1e38c6bc><a class="title" href="/docs/" data-v-1e38c6bc><!--[--><!--]--><!----><span data-v-1e38c6bc>ECE Documentation</span><!--[--><!--]--></a></div></div><div class="content" data-v-2a96a3d0><div class="content-body" data-v-2a96a3d0><!--[--><!--]--><div class="VPNavBarSearch search" data-v-2a96a3d0><!--[--><!----><div id="local-search"><button type="button" aria-label="Search" aria-keyshortcuts="/ control+k meta+k" class="DocSearch DocSearch-Button"><span class="DocSearch-Button-Container"><span class="vpi-search DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key"></kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-2a96a3d0 data-v-39714824><span id="main-nav-aria-label" class="visually-hidden" data-v-39714824> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/docs/" tabindex="0" data-v-39714824 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Home</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/docs/ecelabs.html" tabindex="0" data-v-39714824 data-v-e56f3d57><!--[--><span data-v-e56f3d57>ECE Labs</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/docs/GE133.html" tabindex="0" data-v-39714824 data-v-e56f3d57><!--[--><span data-v-e56f3d57>GE133</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/docs/ROS.html" tabindex="0" data-v-39714824 data-v-e56f3d57><!--[--><span data-v-e56f3d57>ROS</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/docs/turtlebot4.html" tabindex="0" data-v-39714824 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Turtlebot4</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/docs/crazyflie.html" tabindex="0" data-v-39714824 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Crazyflie</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-2a96a3d0 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="true" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-2a96a3d0 data-v-bb2aa2f0 data-v-42cb505d><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-42cb505d><span class="vpi-more-horizontal icon" data-v-42cb505d></span></button><div class="menu" data-v-42cb505d><div class="VPMenu" data-v-42cb505d data-v-25a6cce8><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="true" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-2a96a3d0 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-2a96a3d0><div class="divider-line" data-v-2a96a3d0></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-1df9f90f data-v-8acdfeb5><div class="container" data-v-8acdfeb5><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-8acdfeb5><span class="vpi-align-left menu-icon" data-v-8acdfeb5></span><span class="menu-text" data-v-8acdfeb5>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-8acdfeb5 data-v-0bf0e06f><button data-v-0bf0e06f>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-1df9f90f data-v-e7c6e512><div class="curtain" data-v-e7c6e512></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-e7c6e512><span class="visually-hidden" id="sidebar-aria-label" data-v-e7c6e512> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-8d50c081><section class="VPSidebarItem level-0 has-active" data-v-8d50c081 data-v-d81de50c><!----><div class="items" data-v-d81de50c><!--[--><div class="VPSidebarItem level-1 is-link" data-v-d81de50c data-v-d81de50c><div class="item" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><a class="VPLink link link" href="/docs/GE133/1_EE.html" data-v-d81de50c><!--[--><p class="text" data-v-d81de50c>EE Research Project</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-d81de50c data-v-d81de50c><div class="item" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><a class="VPLink link link" href="/docs/GE133/2_CME.html" data-v-d81de50c><!--[--><p class="text" data-v-d81de50c>CME Research Project</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-1df9f90f data-v-aff0b8d7><div class="VPDoc has-sidebar has-aside" data-v-aff0b8d7 data-v-7011f0d8><!--[--><!--]--><div class="container" data-v-7011f0d8><div class="aside" data-v-7011f0d8><div class="aside-curtain" data-v-7011f0d8></div><div class="aside-container" data-v-7011f0d8><div class="aside-content" data-v-7011f0d8><div class="VPDocAside" data-v-7011f0d8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-60d5052e><div class="content" data-v-60d5052e><div class="outline-marker" data-v-60d5052e></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-60d5052e>On this page</div><ul class="VPDocOutlineItem root" data-v-60d5052e data-v-2d0bdf9b><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-7011f0d8><div class="content-container" data-v-7011f0d8><!--[--><!--]--><main class="main" data-v-7011f0d8><div style="position:relative;" class="vp-doc _docs_GE133_2_CME" data-v-7011f0d8><div><h1 id="cme-research-project" tabindex="-1">CME Research Project <a class="header-anchor" href="#cme-research-project" aria-label="Permalink to “CME Research Project”">​</a></h1><p><span style="font-size:20px;"><strong>Title: Image Classification Using Convolutional Neural Networks</strong></span></p><h2 id="background" tabindex="-1">Background <a class="header-anchor" href="#background" aria-label="Permalink to “Background”">​</a></h2><p>From <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" target="_blank" rel="noreferrer">https://en.wikipedia.org/wiki/Convolutional_neural_network</a>:</p><p>&quot;A convolutional neural network (CNN) is a type of feedforward neural network that learns features via filter (or kernel) optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio. CNNs are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently been replaced—in some cases—by newer deep learning architectures such as the transformer.&quot;</p><h2 id="inception" tabindex="-1">Inception <a class="header-anchor" href="#inception" aria-label="Permalink to “Inception”">​</a></h2><p>&quot;Inception&quot; is a family of CNNs for computer vision, introduced by researchers at Google in 2014 as GoogLeNet (later renamed Inception v1). A team at Google developed the GoogLeNet architecture, an instance of which won the ImageNet Large-Scale Visual Recognition Challenge 2014, trained to identify images into 1,000 different categories. Some example categories include:</p><div class="language-text line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span>Afghan hound</span></span>
<span class="line"><span>African chameleon</span></span>
<span class="line"><span>African crocodile</span></span>
<span class="line"><span>African elephant</span></span>
<span class="line"><span>African grey</span></span>
<span class="line"><span>African hunting dog</span></span>
<span class="line"><span>Airedale</span></span>
<span class="line"><span>American Staffordshire terrier</span></span>
<span class="line"><span>American alligator</span></span>
<span class="line"><span>American black bear</span></span>
<span class="line"><span>American chameleon</span></span>
<span class="line"><span>American coot</span></span>
<span class="line"><span>American egret</span></span>
<span class="line"><span>American lobster</span></span>
<span class="line"><span>Angora</span></span>
<span class="line"><span>Appenzeller</span></span>
<span class="line"><span>Arabian camel</span></span>
<span class="line"><span>Arctic fox</span></span>
<span class="line"><span>Australian terrier</span></span>
<span class="line"><span>Band Aid</span></span>
<span class="line"><span>Bedlington terrier</span></span>
<span class="line"><span>Bernese mountain dog</span></span>
<span class="line"><span>Blenheim spaniel</span></span>
<span class="line"><span>Border collie</span></span>
<span class="line"><span>Border terrier</span></span>
<span class="line"><span>Boston bull</span></span>
<span class="line"><span>Bouvier des Flandres</span></span>
<span class="line"><span>Brabancon griffon</span></span>
<span class="line"><span>Brittany spaniel</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br></div></div><p>To try out Inception using Python, use the &quot;PyTorch&quot; package (<a href="https://pytorch.org/" target="_blank" rel="noreferrer">https://pytorch.org/</a>, assumes you have an up to date version of Python installed):</p><ol><li>Install PyTorch by going to <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noreferrer">https://pytorch.org/get-started/locally/</a> and get the command to run for your environment (e.g., see figure below, for Windows with GPU acceleration &quot;pip3 install torch torchvision --index-url <a href="https://download.pytorch.org/whl/cu126" target="_blank" rel="noreferrer">https://download.pytorch.org/whl/cu126</a>&quot;)</li></ol><p><img src="/docs/assets/pytorch.DHs0wA43.png" alt="PyTorch"></p><ol start="2"><li><p>Download the text file of the 1,000 class names (imagenet_classes.txt) from: <a href="https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt" target="_blank" rel="noreferrer">https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt</a></p></li><li><p>Create Python script &quot;inception.py&quot; using the following code:</p></li></ol><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># File: inception.py</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Description:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#    Use PyTorch (Inception CNN) to classify an input image.  </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#    The top five probability matches are listed.</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#    Derived from code at https://pytorch.org/hub/pytorch_vision_googlenet/</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Copyright (c) 2025, University of Saskatchewan</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sys</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.hub.load(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;pytorch/vision:v0.10.0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;googlenet&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">pretrained</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model.eval()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Get image file name from command line</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">filename </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sys.argv[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># sample execution (requires torchvision)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> PIL</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Image</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torchvision </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> transforms</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">input_image </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Image.open(filename)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">preprocess </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> transforms.Compose([</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    transforms.Resize(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">256</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    transforms.CenterCrop(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">224</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    transforms.ToTensor(),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    transforms.Normalize(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">mean</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.485</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.456</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.406</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">std</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.229</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.224</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.225</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">input_tensor </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> preprocess(input_image)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">input_batch </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> input_tensor.unsqueeze(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># create a mini-batch as expected by the model</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># move the input and model to GPU for speed if available</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.cuda.is_available():</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    input_batch </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> input_batch.to(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model.to(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.no_grad():</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model(input_batch)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Tensor of shape 1000, with confidence scores over ImageNet&#39;s 1000 classes</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#print(output[0])</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># The output has unnormalized scores. To get probabilities, you can run a softmax on it.</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">probabilities </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.nn.functional.softmax(output[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#print(probabilities)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Read the categories</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> open</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;imagenet_classes.txt&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;r&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> f:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    categories </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [s.strip() </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> s </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> f.readlines()]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Show top categories per image</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">top5_prob, top5_catid </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.topk(probabilities, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(top5_prob.size(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(categories[top5_catid[i]], top5_prob[i].item())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br></div></div><ol start="4"><li>Run the code using the following (<code>imageFile</code> should be a JPG or PNG):</li></ol><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> inception.py</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> imageFile</span></span></code></pre></div><h2 id="example" tabindex="-1">Example <a class="header-anchor" href="#example" aria-label="Permalink to “Example”">​</a></h2><p>Image of a dog (right-click and &quot;Save image as...&quot;): <img src="/docs/assets/dog.9jFxtvx7.jpg" alt="Dog"></p><p>The results for this image should look similar to: <img src="/docs/assets/run_dog.D2zvIbm5.png" alt="Run Dog"></p><p>This indicates that the image is a 93.8% match to the &quot;Samoyed&quot; class.</p><h2 id="suggested-research-questions" tabindex="-1">Suggested Research Questions <a class="header-anchor" href="#suggested-research-questions" aria-label="Permalink to “Suggested Research Questions”">​</a></h2><ol><li>How does a Convolution Neural Network work?</li><li>Why is it appropriate for image classification?</li><li>How is it trained?</li><li>How well does it work (try a number of different images)?</li><li>What images does it NOT do well on? Why? How would you fix this?</li></ol><p>Extended topic:</p><ol><li>What is &quot;Transfer Learning&quot;?</li><li>Try using Transfer Learning to add a new image class to an existing CNN (look at finding a simpler starting CNN)</li></ol></div></div></main><footer class="VPDocFooter" data-v-7011f0d8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/docs/GE133/1_EE.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Previous page</span><span class="title" data-v-e257564d>EE Research Project</span><!--]--></a></div><div class="pager" data-v-e257564d><!----></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-1df9f90f data-v-c3855bb3><div class="container" data-v-c3855bb3><p class="message" data-v-c3855bb3>Electrical & Computer Engineering, University of Saskatchewan</p><p class="copyright" data-v-c3855bb3>Copyright © 2025-present Andrew Kostiuk</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"crazyflie_1_basic.md\":\"BRZdzHl7\",\"crazyflie_2_radio.md\":\"52UV3WZM\",\"crazyflie_3_crazyswarm2.md\":\"B2ZA251I\",\"crazyflie_4_control.md\":\"BAFBm1eu\",\"crazyflie_index.md\":\"Dtoht7IG\",\"ecelabs_index.md\":\"B6g0DTLm\",\"ee216_1_lab1.md\":\"Boyo3Uge\",\"ee216_index.md\":\"B-2Ljljp\",\"ee221_index.md\":\"BM65SumD\",\"ee368_1_tf2.md\":\"BksUL1lW\",\"ee368_index.md\":\"Dr4jCYo1\",\"ge133_1_ee.md\":\"VqAS07ys\",\"ge133_2_cme.md\":\"CmeXCLSJ\",\"ge133_index.md\":\"ZgX57qoJ\",\"index.md\":\"D3QWfu3U\",\"ros_1_colcon.md\":\"DGKyJqQ7\",\"ros_2_pubsub.md\":\"CjaaJhH8\",\"ros_3_launch.md\":\"DQhTW2pV\",\"ros_4_rviz.md\":\"tDgh4152\",\"ros_5_gazebo.md\":\"CXAfdU00\",\"ros_6_urdf.md\":\"DerXw-vE\",\"ros_7_sdf.md\":\"CRyhBjK-\",\"ros_index.md\":\"COwtBVG3\",\"turtlebot4_1_status.md\":\"BN6JPo-a\",\"turtlebot4_2_basic.md\":\"BXHtscOO\",\"turtlebot4_3_camera.md\":\"CihXhXDU\",\"turtlebot4_index.md\":\"DbfJQzzR\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"ECE Documentation\",\"description\":\"A VitePress Site\",\"base\":\"/docs/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":\"dark\",\"themeConfig\":{\"nav\":[{\"text\":\"Home\",\"link\":\"/\"},{\"text\":\"ECE Labs\",\"link\":\"/ecelabs\"},{\"text\":\"GE133\",\"link\":\"/GE133\"},{\"text\":\"ROS\",\"link\":\"/ROS\"},{\"text\":\"Turtlebot4\",\"link\":\"/turtlebot4\"},{\"text\":\"Crazyflie\",\"link\":\"/crazyflie\"}],\"outline\":[2,5],\"search\":{\"provider\":\"local\"},\"footer\":{\"message\":\"Electrical & Computer Engineering, University of Saskatchewan\",\"copyright\":\"Copyright © 2025-present Andrew Kostiuk\"},\"sidebar\":{\"/crazyflie/\":[{\"items\":[{\"text\":\"Crazyflie 2.1+ Basics\",\"link\":\"/crazyflie/1_Basic.html\"},{\"text\":\"Crazyflie Radio\",\"link\":\"/crazyflie/2_Radio.html\"},{\"text\":\"Crazyswarm2\",\"link\":\"/crazyflie/3_Crazyswarm2.html\"},{\"text\":\"Crazyflie Control\",\"link\":\"/crazyflie/4_Control.html\"}]}],\"/ee216/\":[{\"items\":[{\"text\":\"Lab 1 - JupyterLab and NumPy\",\"link\":\"/ee216/1_lab1.html\"}]}],\"/ee368/\":[{\"items\":[{\"text\":\"EE368 Lab 1 - TF2\",\"link\":\"/ee368/1_tf2.html\"}]}],\"/GE133/\":[{\"items\":[{\"text\":\"EE Research Project\",\"link\":\"/GE133/1_EE.html\"},{\"text\":\"CME Research Project\",\"link\":\"/GE133/2_CME.html\"}]}],\"/ROS/\":[{\"items\":[{\"text\":\"Build ROS 2 Packages\",\"link\":\"/ROS/1_colcon.html\"},{\"text\":\"Simple Publisher and Subscriber\",\"link\":\"/ROS/2_pubsub.html\"},{\"text\":\"Python Launch Files\",\"link\":\"/ROS/3_launch.html\"},{\"text\":\"RViz\",\"link\":\"/ROS/4_RViz.html\"},{\"text\":\"Gazebo\",\"link\":\"/ROS/5_Gazebo.html\"},{\"text\":\"URDF\",\"link\":\"/ROS/6_URDF.html\"},{\"text\":\"SDFormat\",\"link\":\"/ROS/7_SDF.html\"}]}],\"/turtlebot4/\":[{\"items\":[{\"text\":\"Tutlebot4 Status\",\"link\":\"/turtlebot4/1_Status.html\"},{\"text\":\"Tutlebot4 Basics\",\"link\":\"/turtlebot4/2_Basic.html\"},{\"text\":\"Tutlebot4 Cameras\",\"link\":\"/turtlebot4/3_Camera.html\"}]}]}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false,\"additionalConfig\":{}}");</script>
    
  </body>
</html>